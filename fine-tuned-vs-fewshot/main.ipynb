{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i5o3E0BsyKW"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade torch torchvision transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the emotions datas\n",
        "dataset = load_dataset(\"SetFit/emotion\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "kWYzfsoT2Rrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "xUMvO21v5j-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)"
      ],
      "metadata": {
        "id": "n8l_MM_6693h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "kCOnCVXy7nRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "c3o9Z_fVE7B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "OIGvX_vCea9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "predictions = trainer.predict(tokenized_datasets['test'])\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = tokenized_datasets['test']['label']\n",
        "\n",
        "print(classification_report(true_labels, predicted_labels))\n"
      ],
      "metadata": {
        "id": "SNtv5I65e9La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RVb8S2RD6i1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "REsqWOUH6k0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "encoder = AutoModel.from_pretrained(model_name)\n",
        "encoder.eval()\n",
        "shots = {\n",
        "    \"joy\": [\n",
        "        \"I absolutely loved this!\",\n",
        "        \"This makes me so happy.\"\n",
        "    ],\n",
        "    \"sadness\": [\n",
        "        \"I'm feeling really down today.\",\n",
        "        \"This is so depressing.\"\n",
        "    ],\n",
        "    \"anger\": [\n",
        "        \"This makes me furious!\",\n",
        "        \"I can't stand this.\"\n",
        "    ],\n",
        "    \"fear\": [\n",
        "        \"I'm terrified of what's next.\",\n",
        "        \"This scares me so much.\"\n",
        "    ],\n",
        "    \"surprise\": [\n",
        "        \"Wow, I did not see that coming!\",\n",
        "        \"That's a shocking turn of events.\"\n",
        "    ],\n",
        "    \"love\": [\n",
        "        \"I adore you.\",\n",
        "        \"My heart is full of love.\"\n",
        "    ],\n",
        "}\n"
      ],
      "metadata": {
        "id": "gKInCeN3hsV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_texts(texts):\n",
        "\n",
        "    enc = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        out = encoder(**enc).last_hidden_state\n",
        "    mask = enc.attention_mask.unsqueeze(-1)\n",
        "    summed = (out * mask).sum(1)\n",
        "    lengths = mask.sum(1)\n",
        "    return summed / lengths\n",
        "\n",
        "\n",
        "prototypes = {}\n",
        "for label, examples in shots.items():\n",
        "    emb = embed_texts(examples)\n",
        "    prototypes[label] = emb.mean(0, keepdim=True)\n"
      ],
      "metadata": {
        "id": "Gx55DiUQ4l3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "text = \"I can't believe what a wonderful surprise!\"\n",
        "pred_label, similarities = classify(text)\n",
        "print(f\"â†’ Predicted emotion: {pred_label}\")\n",
        "print(\" Similarities:\", similarities)\n"
      ],
      "metadata": {
        "id": "-DxiJguD4pSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_texts = dataset['test']['text'][:30]\n",
        "true_labels = dataset['test']['label_text'][:30]\n",
        "\n",
        "def classify(text):\n",
        "    q_emb = embed_texts([text])\n",
        "    sims = {\n",
        "        label: F.cosine_similarity(q_emb, proto).item()\n",
        "        for label, proto in prototypes.items()\n",
        "    }\n",
        "\n",
        "    return max(sims, key=sims.get), sims\n",
        "\n",
        "correct = []\n",
        "incorrect = []\n",
        "\n",
        "for text, true_label in zip(test_texts, true_labels):\n",
        "    pred_label, similarities = classify(text)\n",
        "    entry = {\n",
        "        \"text\": text,\n",
        "        \"true_label\": true_label,\n",
        "        \"pred_label\": pred_label,\n",
        "        \"similarities\": similarities\n",
        "    }\n",
        "    if pred_label == true_label:\n",
        "        correct.append(entry)\n",
        "    else:\n",
        "        incorrect.append(entry)\n",
        "\n",
        "print(\"=== CORRECTLY CLASSIFIED ===\")\n",
        "for e in correct:\n",
        "    print(f\"Text: {e['text']}\")\n",
        "    print(f\"True label: {e['true_label']}, Predicted: {e['pred_label']}\")\n",
        "    print(f\"Similarities: {e['similarities']}\\n\")\n",
        "\n",
        "print(\"=== MISCLASSIFIED ===\")\n",
        "for e in incorrect:\n",
        "    print(f\"Text: {e['text']}\")\n",
        "    print(f\"True label: {e['true_label']}, Predicted: {e['pred_label']}\")\n",
        "    print(f\"Similarities: {e['similarities']}\\n\")\n"
      ],
      "metadata": {
        "id": "40haCxMb6nYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}